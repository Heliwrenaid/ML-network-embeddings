{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "collab_main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx6fbGgA--sj",
        "outputId": "ec4b7418-ed72-4335-f13d-637e9b481772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: node2vec in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from node2vec) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from node2vec) (4.64.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from node2vec) (1.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from node2vec) (2.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from node2vec) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->node2vec) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->node2vec) (6.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (6.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip install node2vec tensorflow\n",
        "!pip install gensim \n",
        "from deepwalk_config import *\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optionally load copy\n",
        "!7z x copy.zip -o/content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjRtk7G6YSr0",
        "outputId": "66af7ae8-2e8a-470b-9a7e-01a66c9b7bae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 2254683 bytes (2202 KiB)\n",
            "\n",
            "Extracting archive: copy.zip\n",
            "--\n",
            "Path = copy.zip\n",
            "Type = zip\n",
            "Physical Size = 2254683\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b\n",
            "Would you like to replace the existing file:\n",
            "  Path:     /content/as-rank.caida.peercones-with-IX.txt\n",
            "  Size:     1064687 bytes (1040 KiB)\n",
            "  Modified: 2022-05-27 11:39:30\n",
            "with the file from archive:\n",
            "  Path:     as-rank.caida.peercones-with-IX.txt\n",
            "  Size:     1064687 bytes (1040 KiB)\n",
            "  Modified: 2022-05-27 11:39:30\n",
            "? (Y)es / (N)o / (A)lways / (S)kip all / A(u)to rename all / (Q)uit? A\n",
            "\n",
            "  0% - as-rank.caida.peercones-with-IX.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Folders: 3\n",
            "Files: 12\n",
            "Size:       8937828\n",
            "Compressed: 2254683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-------------- Functions --------------**"
      ],
      "metadata": {
        "id": "zQjUzHbD-txn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choose links from AS file\n",
        "# samples_number for each link type (peer/customer)\n",
        "\n",
        "def get_as_data(as_relation_filename, samples_number):\n",
        "  as_relation_file = open(as_relation_filename, 'r')\n",
        "\n",
        "  peers = 0\n",
        "  customers = 0\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for line in as_relation_file:\n",
        "      if line.startswith('#'):\n",
        "          continue\n",
        "      data = line.split('|')\n",
        "\n",
        "      if '0' in data[2]:\n",
        "          if peers < samples_number:\n",
        "            X.append([data[0], data[1]])\n",
        "            Y.append(data[2])\n",
        "            peers += 1\n",
        "      else:\n",
        "          if customers < samples_number:\n",
        "            X.append([data[0], data[1]])\n",
        "            Y.append(data[2])\n",
        "            customers += 1\n",
        "  return X, Y\n",
        "\n",
        "\n",
        "def get_as_data2(as_relation_filename, samples_number):\n",
        "  as_relation_file = open(as_relation_filename, 'r')\n",
        "\n",
        "  peers = 0\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for line in as_relation_file:\n",
        "      if line.startswith('#'):\n",
        "          continue\n",
        "      data = line.split('|')\n",
        "\n",
        "      if peers < samples_number:\n",
        "        X.append([data[0], data[1]])\n",
        "        Y.append(data[2])\n",
        "        peers += 1\n",
        "       \n",
        "  return X, Y\n",
        "\n",
        "def get_as_data3(as_relation_filename, samples_number, skip):\n",
        "  as_relation_file = open(as_relation_filename, 'r')\n",
        "\n",
        "  peers = 0\n",
        "  skip_ = 0\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for line in as_relation_file:\n",
        "      if line.startswith('#'):\n",
        "          continue\n",
        "\n",
        "      if skip_ < skip:\n",
        "        skip_ += 1\n",
        "        continue\n",
        "        \n",
        "      data = line.split('|')\n",
        "\n",
        "      if peers < samples_number:\n",
        "        X.append([data[0], data[1]])\n",
        "        Y.append(data[2])\n",
        "        peers += 1\n",
        "       \n",
        "  return X, Y\n",
        "\n",
        "\n",
        "\n",
        "import csv\n",
        "\n",
        "# create dataset for neural network in format:\n",
        "# [start_node_embedding], [end_node_embedding], [link_type]\n",
        "def create_dataset(X, Y, X_embed, dataset_filename):\n",
        "\n",
        "  out = open(dataset_filename, 'w')\n",
        "  writer = csv.writer(out)\n",
        "\n",
        "  for i in range(len(X)):\n",
        "      data = []\n",
        "      data.extend(X_embed[X[i][0]])\n",
        "      data.extend(X_embed[X[i][1]])\n",
        "      if '0' in Y[i]:\n",
        "          data.append('0')\n",
        "      else:\n",
        "          data.append('1')\n",
        "      \n",
        "      writer.writerow(data)\n",
        "\n",
        "  out.close()\n",
        "\n",
        "\n",
        "# count links\n",
        "def display_link_stats(Y):\n",
        "\n",
        "  peers = 0\n",
        "  customers = 0\n",
        "\n",
        "  for y in Y:\n",
        "      if int(y) == 0:\n",
        "          peers = peers + 1\n",
        "      else:\n",
        "          customers = customers + 1\n",
        "\n",
        "  print('Peer:', peers, '\\n  Customer:', customers)\n",
        "\n",
        "# embeddings methods ----------------------------------\n",
        "\n",
        "import networkx as nx\n",
        "from node2vec import Node2Vec\n",
        "\n",
        "def node2vec_get_embeddings(as_data):\n",
        "\n",
        "  embedding_filename = \"embeddings_tmp_file\"\n",
        "\n",
        "  # Create a graph \n",
        "  graph = nx.Graph()\n",
        "  for AS in as_data:\n",
        "      graph.add_edge(AS[0], AS[1])\n",
        "\n",
        "  # Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\n",
        "  node2vec = Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=4)  # Use temp_folder for big graphs\n",
        "\n",
        "  # Embed nodes\n",
        "  model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `dimensions` and `workers` are automatically passed (from the Node2Vec constructor)\n",
        "  \n",
        "  # Save embeddings for later use\n",
        "  model.wv.save_word2vec_format(embedding_filename)\n",
        "\n",
        "  embeddings_file = open(embedding_filename,'r')\n",
        "  node_embeddings = {}\n",
        "\n",
        "  i = 0\n",
        "  for line in embeddings_file:\n",
        "      if i == 0:\n",
        "          i = i + 1\n",
        "          continue\n",
        "      data = line.split(' ')\n",
        "      node_embeddings[data[0]] = data[1:]\n",
        "\n",
        "  return node_embeddings\n",
        "\n",
        "\n",
        "def deepwalk_get_embeddings(as_data):\n",
        "\n",
        "  embedding_filename = \"embeddings_tmp_file\"\n",
        " \n",
        " # Create a graph \n",
        "  graph = nx.Graph()\n",
        "  for AS in as_data:\n",
        "      graph.add_edge(AS[0],AS[1])\n",
        "\n",
        "  # Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\n",
        "  model = DeepWalk(graph,walk_length=10,num_walks=80,workers=1)  # Use temp_folder for big graphs\n",
        "  \n",
        "  # Embed nodes\n",
        "  model.train()# train model\n",
        "\n",
        "  # Save embeddings for later use\n",
        "  model.w2v_model.wv.save_word2vec_format(embedding_filename)\n",
        "\n",
        "  embeddings_file = open(embedding_filename,'r')\n",
        "  node_embeddings = {}\n",
        "\n",
        "  i = 0\n",
        "  for line in embeddings_file:\n",
        "      if i == 0:\n",
        "          i = i + 1\n",
        "          continue\n",
        "      data = line.split(' ')\n",
        "      node_embeddings[data[0]] = data[1:]\n",
        "\n",
        "  return node_embeddings"
      ],
      "metadata": {
        "id": "ELREEHbYkPeE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-------------- Create datasets --------------**"
      ],
      "metadata": {
        "id": "tDx5l0L-_TKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create embeddings and basic dataset\n",
        "samples = 2000\n",
        "\n",
        "X, Y = get_as_data('as-rank.caida.peercones-with-IX.txt', samples / 2) # balanced\n",
        "display_link_stats(Y)\n",
        "\n",
        "# X_embed = node2vec_get_embeddings(X)\n",
        "X_embed = deepwalk_get_embeddings(X)\n",
        "create_dataset(X, Y, X_embed, 'dataset_basic.csv')"
      ],
      "metadata": {
        "id": "a7yHNam1_UmR",
        "outputId": "0f018dd3-4694-4780-e12b-a27b946550f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peer: 558 \n",
            "  Customer: 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.5s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning embedding vectors...\n",
            "Learning embedding vectors done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create datasets for training and testing\n",
        "import csv\n",
        "\n",
        "def split_data(X, Y):\n",
        "  \n",
        "  X_0 = []\n",
        "  Y_0 = []\n",
        "\n",
        "  X_1 = []\n",
        "  Y_1 = []\n",
        "\n",
        "  for i in range(len(X)):\n",
        "      if Y[i][0] == '0':\n",
        "        X_0.append(X[i])\n",
        "        Y_0.append([0])\n",
        "      else:\n",
        "        X_1.append(X[i])\n",
        "        Y_1.append([1])\n",
        "        \n",
        "  return X_0, Y_0, X_1, Y_1\n",
        "\n",
        "def write_csv(filename, X, Y):\n",
        "  out = open(filename, 'w')\n",
        "  writer = csv.writer(out)\n",
        "  for i in range(len(X)):\n",
        "    data = []\n",
        "    data.extend(X[i])\n",
        "    data.extend(Y[i])\n",
        "    writer.writerow(data)\n",
        "\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "with open('dataset_basic.csv', mode='r') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "  for row in csv_reader:\n",
        "    X.append(row[0:128])\n",
        "    Y.append(row[128:])\n",
        "    \n",
        "\n",
        "X_0, Y_0, X_1, Y_1 = split_data(X, Y)\n",
        "\n",
        "X.extend(X_0[0:500])\n",
        "X.extend(X_1[0:500])\n",
        "\n",
        "Y.extend(Y_0[0:500])\n",
        "Y.extend(Y_1[0:500])\n",
        "\n",
        "write_csv('dataset_balanced.csv', X, Y)\n",
        "\n",
        "X.clear()\n",
        "Y.clear()\n",
        "X.extend(X_0[500:550])\n",
        "X.extend(X_1[500:550])\n",
        "\n",
        "Y.extend(Y_0[500:550])\n",
        "Y.extend(Y_1[500:550])\n",
        "\n",
        "\n",
        "write_csv('test_dataset_balanced.csv', X, Y)\n",
        "\n",
        "X.clear()\n",
        "Y.clear()\n",
        "\n",
        "X.extend(X_1[550:650])\n",
        "Y.extend(Y_1[550:650])\n",
        "\n",
        "write_csv('test_dataset_unbalanced_one.csv', X, Y)\n",
        "\n",
        "\n",
        "X.clear()\n",
        "Y.clear()\n",
        "\n",
        "X.extend(X_0[500:550])\n",
        "Y.extend(Y_0[500:550])\n",
        "\n",
        "write_csv('test_dataset_unbalanced_zero.csv', X, Y)\n"
      ],
      "metadata": {
        "id": "ro6-mvP8WN5f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**---- Train and evaluate model ----**"
      ],
      "metadata": {
        "id": "wjHJtpN2gniA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create model of neural network\n",
        "\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, input_dim=128, activation='relu'))\n",
        "  model.add(Dense(64, input_dim=128, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # Compile model\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "u2l3-KdHIsw0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create, train and save model\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# load dataset\n",
        "dataframe = read_csv(\"dataset_balanced.csv\", header=None,encoding= 'unicode_escape')\n",
        "# dataframe = shuffle(dataframe)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:128].astype(float)\n",
        "Y = dataset[:,128].astype(int)\n",
        "\n",
        "# evaluate model with standardized dataset\n",
        "n_split=3\n",
        "k = 1\n",
        "\n",
        "for train_index,test_index in KFold(n_splits=n_split, shuffle=True).split(X):\n",
        "\tx_train,x_test=X[train_index],X[test_index]\n",
        "\ty_train,y_test=Y[train_index],Y[test_index]\n",
        "\tvalidation_data = [x_test, y_test];\n",
        "\n",
        "\tprint(\"K =\", k);\n",
        "\tk += 1\n",
        "\n",
        "\tmodel=create_model()\n",
        "\tmodel.fit(x_train, y_train, epochs=20, batch_size=25, validation_data=validation_data, verbose=2)\n",
        "\n",
        "\n",
        "model.save(\"model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jJSZfSHQzFU",
        "outputId": "3997c5de-5577-4b4e-e833-7b9c629837b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K = 1\n",
            "Epoch 1/20\n",
            "69/69 - 1s - loss: 0.2606 - accuracy: 0.9396 - val_loss: 0.0825 - val_accuracy: 0.9766 - 1s/epoch - 15ms/step\n",
            "Epoch 2/20\n",
            "69/69 - 0s - loss: 0.0477 - accuracy: 0.9894 - val_loss: 0.0385 - val_accuracy: 0.9871 - 196ms/epoch - 3ms/step\n",
            "Epoch 3/20\n",
            "69/69 - 0s - loss: 0.0173 - accuracy: 0.9965 - val_loss: 0.0299 - val_accuracy: 0.9930 - 175ms/epoch - 3ms/step\n",
            "Epoch 4/20\n",
            "69/69 - 0s - loss: 0.0076 - accuracy: 0.9988 - val_loss: 0.0320 - val_accuracy: 0.9906 - 188ms/epoch - 3ms/step\n",
            "Epoch 5/20\n",
            "69/69 - 0s - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0338 - val_accuracy: 0.9906 - 177ms/epoch - 3ms/step\n",
            "Epoch 6/20\n",
            "69/69 - 0s - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0174 - val_accuracy: 0.9883 - 196ms/epoch - 3ms/step\n",
            "Epoch 7/20\n",
            "69/69 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9930 - 190ms/epoch - 3ms/step\n",
            "Epoch 8/20\n",
            "69/69 - 0s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 0.9930 - 206ms/epoch - 3ms/step\n",
            "Epoch 9/20\n",
            "69/69 - 0s - loss: 7.0023e-04 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 0.9930 - 179ms/epoch - 3ms/step\n",
            "Epoch 10/20\n",
            "69/69 - 0s - loss: 5.5182e-04 - accuracy: 1.0000 - val_loss: 0.0218 - val_accuracy: 0.9930 - 194ms/epoch - 3ms/step\n",
            "Epoch 11/20\n",
            "69/69 - 0s - loss: 4.4843e-04 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 0.9930 - 201ms/epoch - 3ms/step\n",
            "Epoch 12/20\n",
            "69/69 - 0s - loss: 3.6940e-04 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 0.9930 - 195ms/epoch - 3ms/step\n",
            "Epoch 13/20\n",
            "69/69 - 0s - loss: 3.1143e-04 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9930 - 198ms/epoch - 3ms/step\n",
            "Epoch 14/20\n",
            "69/69 - 0s - loss: 2.6462e-04 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9906 - 194ms/epoch - 3ms/step\n",
            "Epoch 15/20\n",
            "69/69 - 0s - loss: 2.2478e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9930 - 175ms/epoch - 3ms/step\n",
            "Epoch 16/20\n",
            "69/69 - 0s - loss: 1.9359e-04 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9930 - 175ms/epoch - 3ms/step\n",
            "Epoch 17/20\n",
            "69/69 - 0s - loss: 1.6919e-04 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9930 - 173ms/epoch - 3ms/step\n",
            "Epoch 18/20\n",
            "69/69 - 0s - loss: 1.4871e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9930 - 192ms/epoch - 3ms/step\n",
            "Epoch 19/20\n",
            "69/69 - 0s - loss: 1.3186e-04 - accuracy: 1.0000 - val_loss: 0.0242 - val_accuracy: 0.9930 - 189ms/epoch - 3ms/step\n",
            "Epoch 20/20\n",
            "69/69 - 0s - loss: 1.1733e-04 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9930 - 194ms/epoch - 3ms/step\n",
            "K = 2\n",
            "Epoch 1/20\n",
            "69/69 - 1s - loss: 0.2679 - accuracy: 0.9326 - val_loss: 0.0714 - val_accuracy: 0.9812 - 817ms/epoch - 12ms/step\n",
            "Epoch 2/20\n",
            "69/69 - 0s - loss: 0.0406 - accuracy: 0.9906 - val_loss: 0.0263 - val_accuracy: 0.9918 - 189ms/epoch - 3ms/step\n",
            "Epoch 3/20\n",
            "69/69 - 0s - loss: 0.0128 - accuracy: 0.9965 - val_loss: 0.0201 - val_accuracy: 0.9918 - 192ms/epoch - 3ms/step\n",
            "Epoch 4/20\n",
            "69/69 - 0s - loss: 0.0087 - accuracy: 0.9971 - val_loss: 0.0262 - val_accuracy: 0.9918 - 208ms/epoch - 3ms/step\n",
            "Epoch 5/20\n",
            "69/69 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0185 - val_accuracy: 0.9930 - 195ms/epoch - 3ms/step\n",
            "Epoch 6/20\n",
            "69/69 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 0.9930 - 200ms/epoch - 3ms/step\n",
            "Epoch 7/20\n",
            "69/69 - 0s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9930 - 174ms/epoch - 3ms/step\n",
            "Epoch 8/20\n",
            "69/69 - 0s - loss: 9.3474e-04 - accuracy: 1.0000 - val_loss: 0.0249 - val_accuracy: 0.9930 - 208ms/epoch - 3ms/step\n",
            "Epoch 9/20\n",
            "69/69 - 0s - loss: 5.7283e-04 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 0.9930 - 326ms/epoch - 5ms/step\n",
            "Epoch 10/20\n",
            "69/69 - 0s - loss: 4.5381e-04 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 0.9930 - 296ms/epoch - 4ms/step\n",
            "Epoch 11/20\n",
            "69/69 - 0s - loss: 3.6998e-04 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 0.9930 - 348ms/epoch - 5ms/step\n",
            "Epoch 12/20\n",
            "69/69 - 0s - loss: 3.0155e-04 - accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 0.9930 - 264ms/epoch - 4ms/step\n",
            "Epoch 13/20\n",
            "69/69 - 0s - loss: 2.5250e-04 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 0.9930 - 361ms/epoch - 5ms/step\n",
            "Epoch 14/20\n",
            "69/69 - 0s - loss: 2.1409e-04 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 0.9930 - 284ms/epoch - 4ms/step\n",
            "Epoch 15/20\n",
            "69/69 - 0s - loss: 1.9156e-04 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 0.9930 - 367ms/epoch - 5ms/step\n",
            "Epoch 16/20\n",
            "69/69 - 0s - loss: 1.5715e-04 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 0.9930 - 336ms/epoch - 5ms/step\n",
            "Epoch 17/20\n",
            "69/69 - 0s - loss: 1.3667e-04 - accuracy: 1.0000 - val_loss: 0.0319 - val_accuracy: 0.9930 - 402ms/epoch - 6ms/step\n",
            "Epoch 18/20\n",
            "69/69 - 0s - loss: 1.1977e-04 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 0.9930 - 267ms/epoch - 4ms/step\n",
            "Epoch 19/20\n",
            "69/69 - 0s - loss: 1.0645e-04 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 0.9930 - 420ms/epoch - 6ms/step\n",
            "Epoch 20/20\n",
            "69/69 - 0s - loss: 9.4487e-05 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 0.9930 - 386ms/epoch - 6ms/step\n",
            "K = 3\n",
            "Epoch 1/20\n",
            "69/69 - 2s - loss: 0.3063 - accuracy: 0.9220 - val_loss: 0.0869 - val_accuracy: 0.9836 - 2s/epoch - 27ms/step\n",
            "Epoch 2/20\n",
            "69/69 - 0s - loss: 0.0442 - accuracy: 0.9912 - val_loss: 0.0215 - val_accuracy: 0.9965 - 165ms/epoch - 2ms/step\n",
            "Epoch 3/20\n",
            "69/69 - 0s - loss: 0.0159 - accuracy: 0.9965 - val_loss: 0.0173 - val_accuracy: 0.9941 - 192ms/epoch - 3ms/step\n",
            "Epoch 4/20\n",
            "69/69 - 0s - loss: 0.0086 - accuracy: 0.9982 - val_loss: 0.0077 - val_accuracy: 0.9988 - 179ms/epoch - 3ms/step\n",
            "Epoch 5/20\n",
            "69/69 - 0s - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.0043 - val_accuracy: 0.9988 - 168ms/epoch - 2ms/step\n",
            "Epoch 6/20\n",
            "69/69 - 0s - loss: 0.0051 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9977 - 188ms/epoch - 3ms/step\n",
            "Epoch 7/20\n",
            "69/69 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000 - 166ms/epoch - 2ms/step\n",
            "Epoch 8/20\n",
            "69/69 - 0s - loss: 9.5518e-04 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000 - 175ms/epoch - 3ms/step\n",
            "Epoch 9/20\n",
            "69/69 - 0s - loss: 7.2175e-04 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000 - 191ms/epoch - 3ms/step\n",
            "Epoch 10/20\n",
            "69/69 - 0s - loss: 5.6486e-04 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 0.9977 - 179ms/epoch - 3ms/step\n",
            "Epoch 11/20\n",
            "69/69 - 0s - loss: 4.5832e-04 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 0.9977 - 191ms/epoch - 3ms/step\n",
            "Epoch 12/20\n",
            "69/69 - 0s - loss: 3.8010e-04 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 0.9977 - 173ms/epoch - 3ms/step\n",
            "Epoch 13/20\n",
            "69/69 - 0s - loss: 3.1301e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000 - 172ms/epoch - 2ms/step\n",
            "Epoch 14/20\n",
            "69/69 - 0s - loss: 2.6671e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000 - 165ms/epoch - 2ms/step\n",
            "Epoch 15/20\n",
            "69/69 - 0s - loss: 2.3349e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - 188ms/epoch - 3ms/step\n",
            "Epoch 16/20\n",
            "69/69 - 0s - loss: 1.9661e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000 - 190ms/epoch - 3ms/step\n",
            "Epoch 17/20\n",
            "69/69 - 0s - loss: 1.7297e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000 - 191ms/epoch - 3ms/step\n",
            "Epoch 18/20\n",
            "69/69 - 0s - loss: 1.5105e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000 - 189ms/epoch - 3ms/step\n",
            "Epoch 19/20\n",
            "69/69 - 0s - loss: 1.3511e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000 - 179ms/epoch - 3ms/step\n",
            "Epoch 20/20\n",
            "69/69 - 0s - loss: 1.2036e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - 178ms/epoch - 3ms/step\n",
            "INFO:tensorflow:Assets written to: model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate loaded model\n",
        "from tensorflow import keras\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# load dataset\n",
        "dataframe = read_csv(\"test_dataset_balanced.csv\", header=None, encoding= 'unicode_escape')\n",
        "#dataframe = read_csv(\"test_dataset_unbalanced_zero.csv\", header=None)\n",
        "#dataframe = read_csv(\"test_dataset_unbalanced_one.csv\", header=None)\n",
        "\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:128].astype(float)\n",
        "Y = dataset[:,128]\n",
        "\n",
        "display_link_stats(Y)\n",
        "\n",
        "model = keras.models.load_model('model')\n",
        "predictions = (model.predict(X) > 0.5).astype(int)\n",
        "\n",
        "success = 0\n",
        "for i in range(len(X)):\n",
        "  #print(Y[i], \" -> \", predictions[i])\n",
        "  if int(predictions[i]) == int(Y[i]):\n",
        "    success += 1\n",
        "\n",
        "print(\"Result :\", success/len(X)*100)\n"
      ],
      "metadata": {
        "id": "lYH8j-AyTyUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23dc96a5-fc40-4780-d4d3-86cced4fda64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peer: 50 \n",
            "  Customer: 50\n",
            "Result : 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf sample_data\n",
        "!7z a copy.zip /content/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2LWEmtoY0oE",
        "outputId": "f25d8f1a-7b83-43c2-da70-48a186a7920f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Open archive: copy.zip\n",
            "--\n",
            "Path = copy.zip\n",
            "Type = zip\n",
            "Physical Size = 2254683\n",
            "\n",
            "Scanning the drive:\n",
            "  0M Scan  /content/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4 folders, 16 files, 11148430 bytes (11 MiB)\n",
            "\n",
            "Updating archive: copy.zip\n",
            "\n",
            "Items to compress: 20\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b 28% 5 U dataset_balanced.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 6 U dataset_basic.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 6 U dataset_basic.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 8 U embeddings_tmp_file\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 10 U model/saved_model.pb\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Files read from disk: 16\n",
            "Archive size: 4489652 bytes (4385 KiB)\n",
            "Everything is Ok\n"
          ]
        }
      ]
    }
  ]
}
